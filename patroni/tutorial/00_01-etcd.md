# ETCD

etcd is a distributed key-value store that helps applications manage configuration data, service discovery, and distributed coordination. It is designed to be highly available, consistent, and reliable, making it a critical component in cloud-native applications, container orchestration systems like Kubernetes, and distributed systems.

## How Does etcd Work?

etcd runs as a cluster of nodes that communicate with each other to maintain a consistent state. Each node in the cluster stores data in a structured format and keeps a copy of the same data to ensure redundancy and fault tolerance.

When a client writes data to etcd, the change is sent to the leader node, which then replicates it to the other nodes in the cluster. This ensures that all nodes remain synchronized and maintain data consistency.

## Consensus in etcd

At the heart of etcd's reliability is the Raft consensus algorithm. Raft is a protocol designed to ensure that all nodes in the cluster agree on the same data ensuring consistent view of the data, even if some nodes are unavailable or experiencing network issues.

## How Raft Consensus Works

 - Leader Election: At any given time, ONE node is the "leader". The leader is responsible for receiving client requests, proposing changes, and ensuring they're replicated to the other nodes (called "followers"). When an etcd cluster starts, or if the current leader fails, the nodes hold an election to choose a new leader. Each node waits for a random amount of time before sending a vote request to other nodes and the first node to get a majority of votes becomes the new leader. The cluster remains available as long as a majority of nodes (quorum) are still running.

 - Log Replication: When a client wants to change data, it sends the request to the leader. The leader accepts the writes and proposes this change to the followers. The followers vote on the proposal. If a majority of followers agree (including the leader), the change is committed, ensuring consistency. The leader then confirms the change to the client.


### What Happens if There is No Majority?

This is where Raft's strength lies. If a majority of nodes can't communicate (e.g., due to network partitions), no new leader can be elected, and no new changes can be committed.  This prevents the system from getting into an inconsistent state. The system waits for the network to heal and a majority to be re-established.  This is crucial for data integrity.   

Another situation that can happens is if each node votes for itself in an election and no node receives a majority of votes, the election process will restart. This scenario is known as a split vote, and Raft handles it by having each node wait a random amount of time before retrying the election. This random delay helps reduce the chances of repeated ties and ensures that eventually, one node will receive the majority and become the leader.

## Importance of etcd Logs and Performance Considerations

etcd keeps a detailed log of every change made to the data.  These logs are essential for several reasons, including the ensurance of consistency, fault tolerance, leader elections, auditing, and others, maintaining a consistent state across nodes. For example, if a node fails, it can use the logs to catch up with the other nodes and restore its data. The logs also provide a history of all changes, which can be useful for debugging and security analysis if needed.

### Slow Disks Can Be a Problem

etcd is very sensitive to disk I/O performance.  If the disk is slow, writing to the logs (which is a frequent operation) will be slow, which can lead to timeouts, delaying consensus, instability, and even data loss. In extreme cases, slow disk performance can cause a leader to fail health checks, triggering unnecessary leader elections. Always use fast, reliable storage for etcd.  

### Slow or High-Latency Networks

Communication between etcd nodes is critical. A slow or unreliable network can cause delays in replicating data, increasing the risk of stale reads which can trigger premature timeouts leading to leader elections happening more frequently, and even delays in leader elections in some cases, impacting performance and stability. Also keep in mind that if nodes cannot reach each other in a timely manner, the cluster may lose quorum and become unavailable.

## etcd Locks

etcd provides a distributed locking mechanism, which helps applications coordinate actions across multiple nodes and also access to shared resources preventing conflicts. Locks ensure that only one process can hold a resource at a time, avoiding race conditions and inconsistencies. Patroni, for example, uses etcd locks extensively!


## How Patroni Uses etcd Locks for Primary and Election Control

When a Patroni cluster is first started all the PostgreSQL nodes are started as standby nodes. Patroni uses etcd locks to ensure only one standby server is promoted to primary. A Patroni instances attempt to create a lock in etcd at a designated key (e.g., /percona_lab/cluster_1/leader). 

The first instance to successfully acquire the lock with a lease becomes the primary PostgreSQL node and the other instances remain in standby mode, waiting for the lock to be released.

If the current primary node crashes, its lease on the lock expires, and it's interesting to see how the lease ensures that if the node holding the lock crashes, the lock is automatically released after its expiration time. A new election then begins, and a standby node attempts to acquire the lock to become the new primary.

Patroni uses etcd to store the current state of the cluster, ensuring that all nodes are aware of the latest changes. By leveraging etcd locks, Patroni prevents split-brain scenarios and ensures that only one node is the primary at any time, which is crucial for data consistency.

